{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Analyse all house of commons speeches since 1970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[Part 1: Get a list of MPs and their affiliations](MP_speeches-Part1.ipynb)\n",
    "\n",
    "[Part 2: Download all speeches belonging to MPs in list](MP_speeches-Part2.ipynb)\n",
    "\n",
    "[Part 3: Train bigram and trigram models and use them on all speeches](MP_speeches-Part3.ipynb)\n",
    "\n",
    "## Part 4: Train an LDA topic model and process all speeches with it\n",
    "\n",
    "[Part 5: Analyse the results of the LDA model](MP_speeches-Part5.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import gensim\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from config import INTERMEDIATE_DIRECTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Learn the dictionary (list of words) for the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 ms, sys: 0 ns, total: 28 ms\n",
      "Wall time: 45 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/durand/Stuff/Sources/anaconda3/envs/nlp/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-09-22 07:22:47.971573. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to relearn the dictionary.\n",
    "trigram_speeches_filepath = os.path.join(INTERMEDIATE_DIRECTORY, 'trigram_transformed_speeches_all.txt')\n",
    "trigram_dictionary_filepath = os.path.join(INTERMEDIATE_DIRECTORY, 'trigram_dict_all.dict')\n",
    "if False:\n",
    "    trigram_speeches = LineSentence(trigram_speeches_filepath)\n",
    "\n",
    "    # learn the dictionary by iterating over all of the speeches\n",
    "    trigram_dictionary = Dictionary(trigram_speeches)\n",
    "    \n",
    "    # filter tokens that are very rare or too common from\n",
    "    # the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "    trigram_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "    trigram_dictionary.compactify()\n",
    "\n",
    "    trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "else: \n",
    "    # load the finished dictionary from disk\n",
    "    trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Turn speeches into bag-of-words representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/durand/Stuff/Sources/anaconda3/envs/nlp/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-09-22 07:22:50.589193. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "def trigram_bow_generator(filepath):\n",
    "    \"\"\"\n",
    "    generator function to read speeches from a file\n",
    "    and yield a bag-of-words representation\n",
    "    \"\"\"\n",
    "    \n",
    "    for speech in LineSentence(filepath):\n",
    "        yield trigram_dictionary.doc2bow(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.72 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trigram_bow_filepath = os.path.join(INTERMEDIATE_DIRECTORY, 'trigram_bow_corpus_all.mm')\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to build the bag-of-words corpus yourself.\n",
    "if False:\n",
    "    # generate bag-of-words representations for\n",
    "    # all speches and save them as a matrix\n",
    "    MmCorpus.serialize(trigram_bow_filepath,\n",
    "                       trigram_bow_generator(trigram_speeches_filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Train the LDA topic model on the speech corpus\n",
    "If you want the model to use more or fewer topics, then change num_topics below. However, bare in mind that you will have to relabel all the topics yourself if you do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 312 ms, sys: 60 ms, total: 372 ms\n",
      "Wall time: 762 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Train the LDA topic model using Gensim\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "# this is a bit time consuming (takes about 45 mins)- make the if statement True\n",
    "# if you want to train the LDA model yourself.\n",
    "lda_model_filepath = os.path.join(INTERMEDIATE_DIRECTORY, 'lda_model_all')\n",
    "if False:\n",
    "    # load the finished bag-of-words corpus from disk\n",
    "    trigram_bow_corpus = MmCorpus(trigram_bow_filepath)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        \n",
    "        # workers => sets the parallelism, and should be\n",
    "        # set to your number of physical cores minus one\n",
    "        lda = LdaMulticore(trigram_bow_corpus,\n",
    "                           num_topics=100,\n",
    "                           id2word=trigram_dictionary,\n",
    "                           workers=4)\n",
    "    \n",
    "    lda.save(lda_model_filepath)\n",
    "else:\n",
    "    # load the finished LDA model from disk\n",
    "    lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's explore all the topics in the LDA model we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def explore_topic(topic_number, topn=25):\n",
    "    \"\"\"\n",
    "    accept a user-supplied topic number and\n",
    "    print out a formatted list of the top terms\n",
    "    \"\"\"\n",
    "        \n",
    "    print('{:20} {}'.format('term', 'frequency'))\n",
    "\n",
    "    for term, frequency in lda.show_topic(topic_number, topn=topn):\n",
    "        print('{:20} {:.3f}'.format(term, round(frequency, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                 frequency\n",
      "business             0.101\n",
      "small_business       0.018\n",
      "government           0.015\n",
      "regulation           0.013\n",
      "cost                 0.013\n",
      "work                 0.007\n",
      "sector               0.007\n",
      "industry             0.007\n",
      "’s                   0.007\n",
      "small                0.007\n"
     ]
    }
   ],
   "source": [
    "explore_topic(0, topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 0 seems to be about business, judging by the top 10 terms. This method of visualising topics is a bit inconvenient thugh and since we have 100 topics to label, let's use pyLDAvis instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Visualise the LDA model using pyLDAvis\n",
    "pyLDAvis is a smart visualisation app that allows us to navigate through all the topics in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 336 ms, sys: 20 ms, total: 356 ms\n",
      "Wall time: 503 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle\n",
    "import pyLDAvis.gensim\n",
    "from gensim.corpora import  MmCorpus\n",
    "import pyLDAvis\n",
    "import os\n",
    "\n",
    "ldavis_pickle_path = os.path.join(INTERMEDIATE_DIRECTORY, \"pyldavis.p\")\n",
    "# Change to True if you want to recalculate the visualisation (takes about 40 mins)\n",
    "if False:\n",
    "    trigram_bow_corpus = MmCorpus(trigram_bow_filepath)\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda, trigram_bow_corpus,\n",
    "                                                  trigram_dictionary, sort_topics=False)\n",
    "    pickle.dump(LDAvis_prepared, open(ldavis_pickle_path, \"wb\"))\n",
    "else:\n",
    "    LDAvis_prepared = pickle.load(open(ldavis_pickle_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Subtract 1 from topic numbers in visualisation to get pandas index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing topic_names.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/durand/Stuff/Sources/anaconda3/envs/nlp/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-09-22 07:30:59.314511. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "%%writefile topic_names.py\n",
    "# Dictionary of topic names\n",
    "topic_names_100 = {\n",
    "    0: \"business\",\n",
    "    3: \"immigration\",\n",
    "    4: \"counter terrorism\",\n",
    "    5: \"syria\",\n",
    "    6: \"private housing\",\n",
    "    7: \"banking\",\n",
    "    9: \"tribunal\",\n",
    "    18: \"bbc\",\n",
    "    19: \"police force\",\n",
    "    20: \"parliamentary terms\",\n",
    "    21: \"secretary of state terms\",\n",
    "    23: \"local authority\",\n",
    "    25: \"domestic violence\",\n",
    "    26: \"airport and rail expansion\",\n",
    "    27: \"scotland\",\n",
    "    29: \"parliamentary terms+\",\n",
    "    30: \"single market (?)\",\n",
    "    32: \"drugs and alcohol\",\n",
    "    33: \"middle east\",\n",
    "    35: \"care quality commission\",\n",
    "    36: \"speaker of the house\",\n",
    "    39: \"nhs\",\n",
    "    41: \"farming\",\n",
    "    42: \"law\",\n",
    "    43: \"development & climate change\",\n",
    "    44: \"fishing industry\",\n",
    "    45: \"inquiries & reports\",\n",
    "    46: \"northern ireland\",\n",
    "    47: \"construction\",\n",
    "    49: \"animal welfare\",\n",
    "    60: \"fraud terminology\",\n",
    "    62: \"legislation\",\n",
    "    63: \"bill terminology\",\n",
    "    64: \"regional stuff\",\n",
    "    65: \"elections\",\n",
    "    68: \"local services\",\n",
    "    69: \"energy\",\n",
    "    70: \"welfare reforms\",\n",
    "    71: \"european union\",\n",
    "    72: \"education\",\n",
    "    73: \"money-related terms\",\n",
    "    74: \"pensioner income\",\n",
    "    76: \"child poverty\",\n",
    "    78: \"sports & culture\",\n",
    "    79: \"investment\",\n",
    "    84: \"armed forces\",\n",
    "    85: \"economy\",\n",
    "    86: \"house of lords\",\n",
    "    87: \"employee's rights\",\n",
    "    92: \"nuclear weapons\",\n",
    "    95: \"parliamentary terms++\",\n",
    "    99: \"child care\"\n",
    "}\n",
    "\n",
    "def topic_dict(topic_number):\n",
    "    \"\"\"\n",
    "    return name of topic where identified\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        return topic_names_100[topic_number]\n",
    "    except KeyError:\n",
    "        return topic_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Load previously computed bigram and trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/durand/Stuff/Sources/anaconda3/envs/nlp/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-09-22 07:14:27.357676. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 1.61 s, total: 1min 7s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if True:\n",
    "    # Load the bigram and trigram models so we can apply this to any new text\n",
    "    bigram_model_filepath = os.path.join(INTERMEDIATE_DIRECTORY, 'bigram_model_all')\n",
    "    trigram_model_filepath = os.path.join(INTERMEDIATE_DIRECTORY, 'trigram_model_all')\n",
    "    # load the bigram model from disk\n",
    "    bigram_model = gensim.models.Phrases.load(bigram_model_filepath)\n",
    "    # load the trigram model from disk\n",
    "    trigram_model = gensim.models.Phrases.load(trigram_model_filepath)\n",
    "    # Phraser class is much faster so use this instead of Phrase\n",
    "    bigram_phraser = gensim.models.phrases.Phraser(bigram_model)\n",
    "    trigram_phraser = gensim.models.phrases.Phraser(trigram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ...and the helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/durand/Stuff/Sources/anaconda3/envs/nlp/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-09-22 07:11:07.906274. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "# %load helper_functions.py\n",
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_speech(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in speeches from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for speech in f:\n",
    "            yield speech.replace('\\\\n', '\\n')\n",
    "\n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse speeches,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_speech in nlp.pipe(line_speech(filename),\n",
    "                                  batch_size=10000, n_threads=4):\n",
    "        \n",
    "        for sent in parsed_speech.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/durand/Stuff/Sources/anaconda3/envs/nlp/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-09-22 07:11:14.045148. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    # Load english language model from spacy\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en\")\n",
    "\n",
    "def clean_text(speech_text):\n",
    "    \"\"\"\n",
    "    Remove stop words, lemmatize and split into tokens using the trigram parser\n",
    "    and return a bag-of-words representation\n",
    "    \"\"\"\n",
    "    \n",
    "    # parse the review text with spaCy\n",
    "    parsed_speech = nlp(speech_text)\n",
    "    \n",
    "    # lemmatize the text and remove punctuation and whitespace\n",
    "    unigram_speech = [token.lemma_ for token in parsed_speech\n",
    "                      if not punct_space(token)]\n",
    "    \n",
    "    # apply the bigram and trigram phrase models\n",
    "    bigram_speech = bigram_phraser[unigram_speech]\n",
    "    trigram_speech = trigram_phraser[bigram_speech]\n",
    "    \n",
    "    # remove any remaining stopwords\n",
    "    trigram_speech = [term for term in trigram_speech\n",
    "                      if not term in spacy.en.language_data.STOP_WORDS]\n",
    "    \n",
    "    return trigram_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/durand/Stuff/Sources/anaconda3/envs/nlp/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2017-09-22 07:22:10.743816. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "def lda_description(speech_text):\n",
    "    \"\"\"\n",
    "    accept the original text of a speech and (1) parse it with spaCy,\n",
    "    (2) apply text pre-proccessing steps, (3) create a bag-of-words\n",
    "    representation, (4) create an LDA representation, and\n",
    "    (5) print a series containing all the topics and their probabilities in the LDA representation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get clean representation of text\n",
    "    trigram_speech = clean_text(speech_text)\n",
    "\n",
    "     # create a bag-of-words representation\n",
    "    speech_bow = trigram_dictionary.doc2bow(trigram_speech)\n",
    "    \n",
    "    # create an LDA representation\n",
    "    speech_lda = lda[speech_bow]\n",
    "    \n",
    "    topic_dict = dict(zip(range(100), [0.0]*100))\n",
    "    \n",
    "    for topic in speech_lda:\n",
    "        topic_dict[topic[0]] = topic[1]\n",
    "    topic_dict[\"n_words\"] = len(trigram_speech)\n",
    "    return pd.Series(topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Read in details of MPs\n",
    "mps = pd.read_hdf(\"list_of_mps.h5\", \"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply LDA model to all speeches in the speeches dataframe and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.82 s, sys: 15.7 s, total: 22.6 s\n",
      "Wall time: 32.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This takes a while (~3h) so use cached version if available\n",
    "# Change to True if you want to recalculate the LDA topics for the speeches\n",
    "if False:\n",
    "    import dask.dataframe as dd\n",
    "    import dask.threaded\n",
    "    from dask.diagnostics import ProgressBar\n",
    "    pbar = ProgressBar()\n",
    "    pbar.register()\n",
    "    \n",
    "    # You should probably tweak this line to point to all speech dataframes\n",
    "    # I ran this section twice using both sets of raw speeches and saved them to two sections of the processed hdf5 file\n",
    "    speeches = pd.read_hdf(\"raw_speeches.h5\", \"speeches_0\")\n",
    "    speeches = dd.from_pandas(speeches, npartitions=8).map_partitions(lambda x: pd.concat([x, x.body.apply(lda_description, 1)], axis=1)).compute(get=dask.threaded.get)\n",
    "    # And this one to save to the right hdf file. Remember to change it if you don't want to overwrite the file or section.\n",
    "    speeches.to_hdf(\"/media/Stuff/processed_speeches_new.h5\", \"speeches_0\")\n",
    "else:\n",
    "    try:\n",
    "        del speeches\n",
    "    except NameError:\n",
    "            pass\n",
    "    speeches = pd.read_hdf(\"/media/Stuff/processed_speeches_new.h5\", \"speeches_0\").drop(\"body\", axis=1)\\\n",
    "        .append([pd.read_hdf(\"/media/Stuff/processed_speeches_new.h5\", \"speeches_1\").drop(\"body\", axis=1)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "speeches[\"date\"] = pd.to_datetime(speeches[\"date\"])\n",
    "speeches[\"mp_id\"] = pd.to_numeric(speeches[\"mp_id\"])\n",
    "speeches[\"section_id\"] = pd.to_numeric(speeches[\"section_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>debate_title</th>\n",
       "      <th>mp_id</th>\n",
       "      <th>mp_name</th>\n",
       "      <th>n_words</th>\n",
       "      <th>section_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1983-01-24</td>\n",
       "      <td>Oral Answers to Questions &amp;#8212; Trade: Video...</td>\n",
       "      <td>10574</td>\n",
       "      <td>Jack Straw</td>\n",
       "      <td>27.0</td>\n",
       "      <td>16624762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1983-01-24</td>\n",
       "      <td>Oral Answers to Questions &amp;#8212; Trade: Merch...</td>\n",
       "      <td>21914</td>\n",
       "      <td>James Johnson</td>\n",
       "      <td>36.0</td>\n",
       "      <td>16624762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1983-01-24</td>\n",
       "      <td>Oral Answers to Questions &amp;#8212; Trade: Adver...</td>\n",
       "      <td>21960</td>\n",
       "      <td>John Fraser</td>\n",
       "      <td>39.0</td>\n",
       "      <td>16624762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1983-01-24</td>\n",
       "      <td>Oral Answers to Questions &amp;#8212; Trade: Packa...</td>\n",
       "      <td>21960</td>\n",
       "      <td>John Fraser</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16624762</td>\n",
       "      <td>0.251425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1983-01-24</td>\n",
       "      <td>Oral Answers to Questions &amp;#8212; Trade: Video...</td>\n",
       "      <td>21960</td>\n",
       "      <td>John Fraser</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16624762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                       debate_title  mp_id  \\\n",
       "0 1983-01-24  Oral Answers to Questions &#8212; Trade: Video...  10574   \n",
       "1 1983-01-24  Oral Answers to Questions &#8212; Trade: Merch...  21914   \n",
       "2 1983-01-24  Oral Answers to Questions &#8212; Trade: Adver...  21960   \n",
       "3 1983-01-24  Oral Answers to Questions &#8212; Trade: Packa...  21960   \n",
       "4 1983-01-24  Oral Answers to Questions &#8212; Trade: Video...  21960   \n",
       "\n",
       "         mp_name  n_words  section_id         0    1    2    3 ...    90   91  \\\n",
       "0     Jack Straw     27.0    16624762  0.000000  0.0  0.0  0.0 ...   0.0  0.0   \n",
       "1  James Johnson     36.0    16624762  0.000000  0.0  0.0  0.0 ...   0.0  0.0   \n",
       "2    John Fraser     39.0    16624762  0.000000  0.0  0.0  0.0 ...   0.0  0.0   \n",
       "3    John Fraser     13.0    16624762  0.251425  0.0  0.0  0.0 ...   0.0  0.0   \n",
       "4    John Fraser     23.0    16624762  0.000000  0.0  0.0  0.0 ...   0.0  0.0   \n",
       "\n",
       "    92   93   94   95   96   97   98   99  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 106 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
